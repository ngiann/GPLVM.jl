# There are two cases cases:

# 1. At testing no error measurements provided: assume at training GPLVMâ‚Š precision was optimised.

function inferlatent(Xâ‚Š, R; iterations = 10, repeats=1) 
    
    @assert isa(R[:ğ›ƒ], FillArrays.AbstractFillMatrix)

    infertestlatent(Xâ‚Š, Fill(R[:ğ›ƒ][1], size(Xâ‚Š)); Î¼ = R[:Î¼], Î£ = R[:Î£], K = R[:K], Î· = R[:Î·], Î›root = R[:Î›root], net = R[:net], w = R[:w],
    Î± = R[:Î±], b = R[:b], Z = R[:Z], Î¸ = R[:Î¸], JITTER = R[:JITTER], rg = R[:rg], iterations = iterations, repeats = repeats)

end

# 2. At testing error measuments are provided.

inferlatent(Xâ‚Š, ğ›”, R; iterations = iterations, repeats = repeats) = infertestlatent(Xâ‚Š, inverterrors(ğ›”);  Î¼ = R[:Î¼], Î£ = R[:Î£], K = R[:K], Î· = R[:Î·], Î›root = R[:Î›root], net = R[:net], w = R[:w],
Î± = R[:Î±], b = R[:b], Z = R[:Z], Î¸ = R[:Î¸], JITTER = R[:JITTER], rg = R[:rg], iterations = iterations, repeats = repeats)



function infertestlatent(Xâ‚Š, ğ›ƒ; Î¼ = Î¼, Î£ = Î£, K = K, Î· = Î·, Î›root = Î›root, net = net, w = w,
                             Î± = Î±, b = b, Z = Z, Î¸ = Î¸, JITTER = JITTER, rg = rg, iterations = iterations, repeats = repeats)

    # sort out dimensions

    D, N = size(Î¼); @assert(N == size(Z, 2) == size(Î›root, 1) == size(Î›root, 2))

    Q = size(Z, 1)

    Nâ‚Š = size(Xâ‚Š, 2); @assert(D == size(Xâ‚Š, 1)); @assert(size(Xâ‚Š) == size(ğ›ƒ))


    # pre-calculate
    
    inv_K_plus_Î›â»Â¹ = aux_invert_K_plus_Î›â»Â¹(K=K, Î›root=Î›root)

    inv_K_mul_Î¼ = K\Î¼'


    # convenient, shorter name

    unpack(p) = unpack_inferlatent_gplvmplus(p ; Q = Q, Nâ‚Š = Nâ‚Š, w = w, net = net)
    

    #--------------------------------------------------
    function objective(Zâ‚Š, Î½, Lroot)
    #--------------------------------------------------

        # return partial log-likelihood composed of sum of log-prior contribution, entropy, penalty on latent coordinates

        local â„“, A = partial_objective(Zâ‚Š, Î½, Lroot; Z = Z, Î¸ = Î¸, JITTER = JITTER, Î· = Î·, D = D, inv_K_plus_Î›â»Â¹ = inv_K_plus_Î›â»Â¹, inv_K_mul_Î¼ = inv_K_mul_Î¼)

        # log-likelihood contribution

        local E, V = expectation_latent_function_values(;Î± = Î±, b = b, Î¼ = Î½, Î£ = A)

        â„“ += -0.5*D*Nâ‚Š*log(2Ï€) + 0.5*sum(log.(ğ›ƒ))  -0.5*sum(ğ›ƒ .* abs2.(myskip.(Xâ‚Š .- E))) - 1/2 * sum(ğ›ƒ .* V)

        return â„“

    end


    # initialise parameters randomly
    @assert(Nâ‚Š == 1)

    function p0()

        local luckyindex = ceil(Int, rand(rg)*(size(Z,2))) 
        
        [Z[:,luckyindex]; randn(rg, Nâ‚Š)]

    end


    #-----------------------------------------------------------------
    # define options, loss and gradient to be passed to Optim.optimize
    #-----------------------------------------------------------------

    opt = Optim.Options(iterations = iterations, show_trace = false, show_every = 1)

    objective(p) = -objective(unpack(p)...)

    fg! = getfg!(objective)


    #-----------------------------------------------------------------
    # Carry out actual optimisation and obtain optimised parameters
    #-----------------------------------------------------------------

    @printf("Optimising %d number of parameters\n",length(p0()))

    solutions = [optimize(Optim.only_fg!(fg!), p0(), ConjugateGradient(), opt) for _ in 1:repeats]

    bestindex = argmin([s.minimizer for s in solutions])

    Zopt, Î½opt, Lroot = unpack(solutions[bestindex].minimizer)
   
    return Zopt

end