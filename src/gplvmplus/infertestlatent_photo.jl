infertestlatent_photo(U,B,S,R; iterations = 1000, repeats = 1) = infertestlatent_photo(U, B, S; R..., iterations = iterations, repeats = repeats)

function infertestlatent_photo(U, B, S; Œº = Œº, Œ£ = Œ£, K = K, Œ∑ = Œ∑, Œõroot = Œõroot, net = net, w = w,
                             Œ± = Œ±, b = b, ùõÉ = ùõÉ, Z = Z, Œ∏ = Œ∏, JITTER = JITTER, rg = rg, iterations = iterations, repeats = repeats)

    # work out and verify dimensions
    D, N = size(Œº); @assert(N == size(Z, 2) == size(Œõroot, 1) == size(Œõroot, 2))

    Q = size(Z, 1)

    J, N‚Çä = size(U); @assert(size(B,1) == J); @assert(size(B, 2) == D); @assert(size(S) == size(U))


    # pre-calculate
    
    inv_K_plus_Œõ‚Åª¬π = aux_invert_K_plus_Œõ‚Åª¬π(K=K, Œõroot=Œõroot)

    inv_K_mul_Œº = K\Œº'


    # convenient, shorter name

    unpack(p) = unpack_inferlatent_gplvmplus(p ; Q = Q, N‚Çä = N‚Çä, w = w, net = net)


    #--------------------------------------------------
    function objective(Z‚Çä, ŒΩ, Lroot)
    #--------------------------------------------------

        # return partial log-likelihood composed of sum of log-prior contribution, entropy, penalty on latent coordinates

        local ‚Ñì, A = partial_objective(Z‚Çä, ŒΩ, Lroot; Z = Z, Œ∏ = Œ∏, JITTER = JITTER, Œ∑ = Œ∑, D = D, inv_K_plus_Œõ‚Åª¬π = inv_K_plus_Œõ‚Åª¬π, inv_K_mul_Œº = inv_K_mul_Œº)

        # log-likelihood contribution

        for t in 1:N‚Çä, j in 1:J
            
            # local aux = zero(eltype(ŒΩ))
            # for d in 1:D
            #     aux += B[j,d] * E(a = Œ±, Œº = ŒΩ[d,t], œÉ = sqrt(A[t,t]),b = b)
            # end
            
            # line below implements commented-out code above
            auxE = sum( B[j,:] .* exp.(Œ±*ŒΩ[:,t]   .+     Œ±^2*A[t,t] / 2 .+  b) )

            ‚Ñì += logpdf(Normal(auxE, S[j,t]), U[j,t])

            # local aux_tr = zero(eltype(ŒΩ))
            # for d in 1:D
            #     aux_tr += B[j,d]^2 * V(a = Œ±, Œº = ŒΩ[d,t], œÉ = sqrt(A[t,t]),b = b)
            # end

            # line below implements commented-out code above. Use property V[X] = E[X¬≤] - (E[X])¬≤
            aux_V = sum( B[j,:].^2 .* (exp.(2*Œ±*ŒΩ[:,t] .+ (2*Œ±)^2*A[t,t] / 2 .+ 2b) .- (exp.(Œ±*ŒΩ[:,t]   .+     Œ±^2*A[t,t] / 2 .+  b)).^2) )

        
            ‚Ñì += -(0.5 / S[j,t]^2) * aux_V

        end

        return ‚Ñì
        
    end


    #-----------------------------------------------------------------
    # initialise parameters by picking random coordinate and random
    # values for the sqrt diagonal parametrising the posterior cov
    #-----------------------------------------------------------------

    @assert(N‚Çä == 1)

    function p0()

        local luckyindex = ceil(Int, rand(rg)*(size(Z,2))) 
        
        [Z[:,luckyindex]; randn(rg, N‚Çä)]

    end
    
    #-----------------------------------------------------------------
    # define options, loss and gradient to be passed to Optim.optimize
    #-----------------------------------------------------------------

    opt = Optim.Options(iterations = iterations, show_trace = true, show_every = 1)

    objective(p) = -objective(unpack(p)...)

    fg! = getfg!(objective)
    
    
    #-----------------------------------------------------------------
    # Carry out actual optimisation and obtain optimised parameters
    #-----------------------------------------------------------------

    @printf("Optimising %d number of parameters\n",length(p0()))

    solutions = [optimize(Optim.only_fg!(fg!), p0(), ConjugateGradient(), opt) for _ in 1:repeats] # alphaguess = InitialQuadratic(Œ±0=1e-8)

    bestindex = argmin([s.minimum for s in solutions])

    Zopt, _ŒΩopt, _Lroot = unpack(solutions[bestindex].minimizer)
    
    return Zopt
    
end