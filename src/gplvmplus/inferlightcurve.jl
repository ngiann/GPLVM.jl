inferlightcurve(tobs,U,B,S,R) = inferlightcurve_(tobs, U, B, S; R...)

function inferlightcurve_(tobs, U, B, S; Œº = Œº, Œ£ = Œ£, K = K, Œ∑ = Œ∑, Œõroot = Œõroot, net = net, w = w,
                             Œ± = Œ±, b = b, ùõÉ = ùõÉ, Z = Z, Œ∏ = Œ∏, JITTER = JITTER, rg = rg)

    # work out and verify dimensions
    D, N = size(Œº); @assert(N == size(Z, 2) == size(Œõroot, 1) == size(Œõroot, 2))

    Q = size(Z, 1)

    J, T = size(U); @assert(size(B,1) == J); @assert(size(B, 2) == D); @assert(size(S) == size(U))

    # set up RBF network
    rbf  = GPLVM.RBF(10) ### ‚ùó note fixed number of basis functions in rbf network ‚ùó
    nwts = numweights(rbf)
    Œ∂    = 2*((sort(tobs) .- minimum(tobs)) / (maximum(tobs) - minimum(tobs))) .- 1.0

    # precalculate

    invK_mul_Œº = (K\Œº')

    inv_of_K_plus_Œõ‚Åª¬π = aux_invert_K_plus_Œõ‚Åª¬π(K=K, Œõroot=Œõroot)

    #--------------------------------------------------
    function unpack(p)
    #--------------------------------------------------

        local MARK = 0

        local rbfweights = reshape(p[MARK+1:MARK+nwts*Q], nwts, Q); MARK += Q*nwts
        
        local Lroot = Diagonal(p[MARK+1:MARK+T]); MARK += T

        local c = exp(p[MARK+1]); MARK += 1
        
        @assert(MARK == length(p)) # all parameters must be used up

        local Z‚Çä = rbf(Œ∂, rbfweights, 0.5)' ### ‚ùó note fixed width of rbf network ‚ùó
       
        local ŒΩ = net(w, Z‚Çä)

        return Z‚Çä, ŒΩ, Lroot, c, rbfweights

    end

    

    #--------------------------------------------------
    function objective(Z‚Çä, ŒΩ, Lroot, c, wrbf)
    #--------------------------------------------------

        # Calculate cross-covariance matrix between test and training inputs
        local K‚Çä = covariance(pairwise(SqEuclidean(), Z‚Çä, Z), Œ∏); @assert(size(K‚Çä, 1) == T)

        # Calculate "self"-covariance matrix between test inputs
        local K‚Çä‚Çä = Symmetric(covariance(pairwise(SqEuclidean(), Z‚Çä), Œ∏) + JITTER*I); @assert(size(K‚Çä‚Çä, 1) == T)
        
        # calculate mean of "prior" of test latent function values
        local m = (K‚Çä*(invK_mul_Œº))'
       
        @assert(size(m, 1) == D); @assert(size(m, 2) == T)

        # calculate covariance of "prior" of test latent function values
        local C = K‚Çä‚Çä - K‚Çä*inv_of_K_plus_Œõ‚Åª¬π*K‚Çä'; @assert(size(C, 1) == T); @assert(size(C, 2) == T);


        # calculate posterior covariance of test latent function values
        local A = aux_invert_K‚Åª¬π_plus_Œõ(K=Symmetric(C+JITTER*I) , Œõroot = Lroot)

        # log-prior contribution
        local ‚Ñì = zero(eltype(Z‚Çä))

        # local C·µ§ = cholesky(Symmetric(C)).L
        # ‚Ñì += -0.5*D*T*log(2œÄ) - 0.5*sum(abs2.(C·µ§\(ŒΩ-m)')) - D*0.5*tr(C\A) - D*sum(log.(diag(C·µ§)))

        ‚Ñì +=  expectation_of_sum_D_log_prior_zero_mean(;K = Symmetric(C), Œº = (ŒΩ-m), Œ£ = A)


        # log-likelihood contribution

        for t in 1:T, j in 1:J
            
            
            # local aux = 0.0
            # for d in 1:D
            #     aux += c * B[j,d] * ExponentialExpectations.E(a = Œ±, Œº = ŒΩ[d,t], œÉ = sqrt(A[t,t]),b = b)
            # end
            # ‚Ñì += logpdf(Normal(aux, S[j,t]), U[j,t])
            
            # following two lines implement above commented out block - keep above for numerical verification

            Ef = exp.(Œ±*ŒΩ[:,t] .+ Œ±^2*A[t,t] / 2 .+  b)
            
            ‚Ñì += logpdf(Normal(c*sum(B[j,:].*Ef), S[j,t]), U[j,t])

            # local aux_tr = 0.0
            # for d in 1:D
            #     aux_tr += c^2 * B[j,d]^2 * ExponentialExpectations.V(a = Œ±, Œº = ŒΩ[d,t], œÉ = sqrt(A[t,t]),b = b)
            # end
            # ‚Ñì +=  (1 / (2*S[j,t]^2)) * aux_tr

            # following let block implements above commented out block - keep above for numerical verification
            local Vterm = let
                
                local Ef¬≤ = exp.(2*Œ±*ŒΩ[:,t] .+ (2*Œ±)^2*A[t,t] / 2 .+ 2b) 

                local V = Ef¬≤ .- Ef.^2 # this is V[X] = E[X¬≤] - (E[X])¬≤ # There may be a computational gain to be had here

                c^2 * sum( B[j,:].^2 .* V)

            end
        

            ‚Ñì +=  - (1 / (2*S[j,t]^2)) * Vterm

        end

        # entropy contribution with constants discarded
        ‚Ñì += 0.5*D*logabsdet(A)[1] 

        # penalty on rbf weights - not in latex
        ‚Ñì += - 0.5*Œ∑*sum(abs2.(wrbf)) - 0.5*Œ∑*sum(abs2.(Z‚Çä)) 

        return ‚Ñì
    end


    #-----------------------------------------------------------------
    # initialise parameters, define options, loss and gradient
    #-----------------------------------------------------------------
   
    p0 = [randn(rg, Q*nwts)*0.5; randn(rg, T); 0.0]

    opt = Optim.Options(iterations = 1000, show_trace = true, show_every = 1)

    objective(p) = -objective(unpack(p)...)

    function fg!(F, G, x)
            
        value, ‚àáf = Zygote.withgradient(objective,x)

        isnothing(G) || copyto!(G, ‚àáf[1])

        isnothing(F) || return value

        nothing

    end


    #-----------------------------------------------------------------
    # Carry out actual optimisation and obtain optimised parameters
    #-----------------------------------------------------------------

    @printf("Optimising %d number of parameters\n",length(p0))

    # p1 = let 

    #     local nopt = Optim.Options(iterations = 100, show_trace = true, show_every = 100)

    #     optimize(objective, p0, NelderMead(), nopt).minimizer

    # end


    @printf("Optimising %d number of parameters\n",length(p0))
    
    bnd = [[(-3,3) for _ in 1:Q*nwts]; [(-10,50) for _ in 1:T]; (-4,5)]
    p1 = best_candidate(bboptimize(objective, p0; Method=:generating_set_search, SearchRange = bnd, NumDimensions = length(p0), MaxFuncEvals = 200_000))

    results = optimize(objective, p1, LBFGS(), opt, autodiff=:forward).minimizer

    Zopt, ŒΩopt, Lroot = unpack(results)
   
    return Zopt

end