function gplvmvar_pos(X, ğ›” = missing; iterations = 1, H1 = 10, H2 = H1, seed = 1, Q = 2, JITTER = 1e-6, Î· = 1e-2, VERIFY = false)

    #---------------------------------------------------------------------------
    # Setup variables and free parameters: set random seed, get dimensions, etc
    #---------------------------------------------------------------------------

    # fix random seed for reproducibility

    rg = MersenneTwister(seed)

    # auxiliary type for dispatching to appropriate method

    modeltype = Val(:gplvmvarnet_pos)

    # get dimensions of data

    D, N = size(X)
    
    # report to user

    @printf("Running gplvmvarnet_pos.\n")
    @printf("There are %d number of data items\n", N)
    @printf("There are %d number of dimensions\n", D)
    @printf("Q=%d\n", Q)

    # we work with precisions instead of standard deviations

    ğ›ƒ = inverterrors(ğ›”)


    # define neural network that modeltypes variational parameters and its number of weights

    net = ThreeLayerNetwork(in = Q, H1 = H1, H2 = H2, out=D)
    
    nwts = ForwardNeuralNetworks.numweights(net)
  
    
    # initialise parameters randomly

    p0 = let
        
        ismissing(ğ›ƒ) ? [randn(rg, Q*N)*0.2; randn(rg,2)*1; 0.2*randn(rg, nwts); randn(rg, N); randn(rg, 2)] : 
                       [randn(rg, Q*N)*0.2; randn(rg,1)*1; 0.2*randn(rg, nwts); randn(rg, N); randn(rg, 2)]

    end

    # define auxiliary unpack function

    upk(p, ğ›ƒ) = unpack(modeltype, p, ğ›ƒ, D, N, net, Q)


    #-----------------------------------------------------------------
    # define options, loss and gradient to be passed to Optim.optimize
    #-----------------------------------------------------------------

    opt = Optim.Options(iterations = iterations, show_trace = true, show_every = 1)

    objective(p) = -marginallikelihood(modeltype, X, upk(p, ğ›ƒ)...; JITTER = JITTER, Î· = Î·)

    function fg!(F, G, x)
            
        value, âˆ‡f = Zygote.withgradient(objective, x)

        isnothing(G) || copyto!(G, âˆ‡f[1])

        isnothing(F) || return value

        nothing

    end

    
    #-----------------------------------------------------------------
    # Carry out actual optimisation and obtain optimised parameters
    #-----------------------------------------------------------------

    @printf("Optimising %d number of parameters\n",length(p0))

    VERIFY ? numerically_verify(modeltype, X, upk(p0, ğ›ƒ)..., JITTER, Î·) : nothing
    
    results = optimize(Optim.only_fg!(fg!), p0, ConjugateGradient(), opt) # alphaguess = InitialQuadratic(Î±0=1e-8)

    VERIFY ? numerically_verify(modeltype, X, upk(results.minimizer, ğ›ƒ)..., JITTER, Î·) : nothing

    Zopt, Î¸opt, ğ›ƒopt, Î¼opt, Î›rootopt, wopt, Î±opt, bopt = upk(results.minimizer, ğ›ƒ)
   

    #-----------------------------------------------------------------
    # Return optimised latent coordinates and other parameters
    #-----------------------------------------------------------------

    return let 

        local DÂ²    = pairwise(SqEuclidean(), Zopt)

        local Kopt  = Symmetric(covariance(DÂ², Î¸opt) + JITTER*I)

        local Î£opt  = aux_invert_Kâ»Â¹_plus_Î›(;K = Kopt, Î›root = Î›rootopt) + JITTER*I

        (Î¼ = Î¼opt, Î£ = Î£opt, K = Kopt, Î· = Î·, Î›root = Î›rootopt, net = net, w = wopt,
         Î± = Î±opt, b = bopt, ğ›ƒ = ğ›ƒopt, Z = Zopt, Î¸ = Î¸opt, JITTER = JITTER, rg = rg)
    end

    
    
end