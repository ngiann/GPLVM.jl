function marginallikelihood(::Val{:gplvmvarnet_pos}, X, Z, Î¸, ğ›ƒ, Î¼, Î›root, w, Î±, b; JITTER = JITTER, Î· = Î·)

    # sort out and verify dimensions

    N = size(Î›root, 1); @assert(size(Î¼, 2) == size(Z, 2) == size(X, 2) == N)

    D = size(X, 1); @assert(size(Î¼, 1) == D)

    
    # calculate prior and posterior covariance matrices K and Î£

    DÂ² = pairwise(SqEuclidean(), Z)

    K  = Symmetric(covariance(DÂ², Î¸) + JITTER*I)

    Î£  = aux_invert_Kâ»Â¹_plus_Î›(;K = K, Î›root = Î›root) + JITTER*I


    # log-prior contribution - implements equation eq:log_prior_gplvm_pos

    â„“ = expectation_of_sum_D_log_prior_zero_mean(;K = K, Î¼ = Î¼, Î£ = Î£) # â—check what happens with missing observations.


    # log-likelihood contribution - implements eq:log_likel_gplvm_pos

    E, V = expectation_latent_function_values(;Î± = Î±, b = b, Î¼ = Î¼, Î£ = Î£)

    countObs = count(x -> ~ismissing(x), X)

    â„“ += - 0.5*countObs*log(2Ï€) + 0.5*sum(myskip.(log.(ğ›ƒ))) - 0.5*sum(ğ›ƒ .* abs2.(myskip.(X .- E))) - 1/2 * sum(myskip.(ğ›ƒ .* V))


    # entropy contribution with constants discarded - implements eq:entropy_gplvm_pos

    â„“ += D*entropy(Î£) # note multiplication with D


    # penalty on latent coordinates - not in latex

    â„“ += - 0.5*Î·*sum(abs2.(Z))


    # penalty on neural network weights to smoothen optimisation - not in latex

    â„“ += - 0.5*Î·*sum(abs2.(w))

    return â„“

end
