function marginallikelihood(::Val{:gplvmvarnet_pos}, X, Z, Î¸, ğ›ƒ, Î¼, Î›root, w, Î±, b; JITTER = JITTER, Î· = Î·)

    # sort out dimensions

    local N = size(Î›root, 1); @assert(size(Î¼, 2) == size(Z, 2) == size(X, 2) == N)

    local D = size(X, 1); @assert(size(Î¼, 1) == D)

    
    # calculate prior and posterior covariance matrices K and Î£

    local DÂ² = pairwise(SqEuclidean(), Z)

    local K  = Symmetric(covariance(DÂ², Î¸) + JITTER*I)

    local Î£  = aux_invert_Kâ»Â¹_plus_Î›(;K = K, Î›root = Î›root)+ JITTER*I


    # log-prior contribution - implements equation eq:log_prior_gplvm_pos

    local â„“ = expectation_of_sum_D_log_prior_zero_mean(;K = K, Î¼ = Î¼, Î£ = Î£)


    # log-likelihood contribution - implements eq:log_likel_gplvm_pos

    local E, V = expectation_latent_function_values(;Î± = Î±, b = b, Î¼ = Î¼, Î£ = Î£)

    â„“ += - 0.5*D*N*log(2Ï€) + 0.5*sum(log.(ğ›ƒ)) - 0.5*sum(ğ›ƒ .* abs2.(myskip.(X .- E))) - 1/2 * sum(ğ›ƒ .* V)


    # entropy contribution with constants discarded - implements eq:entropy_gplvm_pos

    â„“ += 0.5*D*logabsdet(Î£)[1] 


    # penalty on latent coordinates - not in latex

    â„“ += - 0.5*Î·*sum(abs2.(Z))


    # penalty on neural network weights to smoothen optimisation - not in latex

    â„“ += - 0.5*Î·*sum(abs2.(w))

    return â„“

end
