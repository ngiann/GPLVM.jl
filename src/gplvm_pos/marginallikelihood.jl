function marginallikelihood(::Val{:gplvmvarnet_pos}, X, Z, Î¸, ğ›ƒ, Î¼, Î›root, w, Î±, b; JITTER = JITTER, Î· = Î·)

    # sort out dimensions

    local N = size(Î›root, 1); @assert(size(Î¼, 2) == size(Z, 2) == size(X, 2) == N)

    local D = size(X, 1); @assert(size(Î¼, 1) == D)

    # calculate prior and posterior covariance matrices K and Î£

    local DÂ² = pairwise(SqEuclidean(), Z)

    local K  = Symmetric(covariance(DÂ², Î¸) + JITTER*I)

    local Î£  = aux_invert_Kâ»Â¹_plus_Î›(;K = K, Î›root = Î›root)+ JITTER*I

    local U = cholesky(K).L
    
    # accummulate here marginal log likelihood

    local â„“ = zero(eltype(Z))


    # log-prior contribution - implements equation eq:log_prior_gplvm_pos

    â„“ += - 0.5*sum(abs2.(U\Î¼')) - 0.5*D*N*log(2Ï€) - D*sum(log.(diag(U))) - 0.5*D*tr(U'\(U\Î£)) # tr(U'\(U\Î£)) equiv to tr(K\Î£)


    # log-likelihood contribution - implements eq:log_likel_gplvm_pos

    local E = exp.(Î±*Î¼   .+     Î±^2*diag(Î£)' / 2 .+  b)

    local B = exp.(2*Î±*Î¼ .+ (2*Î±)^2*diag(Î£)' / 2 .+ 2b) 

    local V = B .- E.^2 # this is V[X] = E[XÂ²] - (E[X])Â² # There  may be a computational gain here

    â„“ += - 0.5*D*N*log(2Ï€) + 0.5*sum(log.(ğ›ƒ)) - 0.5*sum(ğ›ƒ .* abs2.(myskip.(X .- E))) - 1/2 * sum(ğ›ƒ .* V)


    # entropy contribution with constants discarded - implements eq:entropy_gplvm_pos

    â„“ += 0.5*D*logabsdet(Î£)[1] 


    # penalty on latent coordinates - not in latex

    â„“ += - 0.5*Î·*sum(abs2.(Z))


    # penalty on neural network weights to smoothen optimisation - not in latex

    â„“ += - 0.5*Î·*sum(abs2.(w))

    return â„“

end
