# There are two cases cases:

# 1. At testing no error measurements provided: assume at training GPLVM‚Çä precision was optimised.

function inferlatent(X‚Çä, R; iterations = 10, repeats=1) 
    
    @assert isa(R[:ùõÉ], FillArrays.AbstractFillMatrix)

    infertestlatent(X‚Çä, Fill(R[:ùõÉ][1], size(X‚Çä)); Œº = R[:Œº], Œ£ = R[:Œ£], K = R[:K], Œ∑ = R[:Œ∑], Œõroot = R[:Œõroot], net = R[:net], w = R[:w],
    Œ± = R[:Œ±], b = R[:b], Z = R[:Z], Œ∏ = R[:Œ∏], JITTER = R[:JITTER], rg = R[:rg], iterations = iterations, repeats = repeats)

end

# 2. At testing error measuments are provided.

inferlatent(X‚Çä, ùõî, R; iterations = iterations, repeats = repeats) = infertestlatent(X‚Çä, inverterrors(ùõî);  Œº = R[:Œº], Œ£ = R[:Œ£], K = R[:K], Œ∑ = R[:Œ∑], Œõroot = R[:Œõroot], net = R[:net], w = R[:w],
Œ± = R[:Œ±], b = R[:b], Z = R[:Z], Œ∏ = R[:Œ∏], JITTER = R[:JITTER], rg = R[:rg], iterations = iterations, repeats = repeats)



function infertestlatent(X‚Çä, ùõÉ; Œº = Œº, Œ£ = Œ£, K = K, Œ∑ = Œ∑, Œõroot = Œõroot, net = net, w = w,
                             Œ± = Œ±, b = b, Z = Z, Œ∏ = Œ∏, JITTER = JITTER, rg = rg, iterations = iterations, repeats = repeats)

    # sort out dimensions

    D, N = size(Œº); @assert(N == size(Z, 2) == size(Œõroot, 1) == size(Œõroot, 2))

    Q = size(Z, 1)

    N‚Çä = size(X‚Çä, 2); @assert(D == size(X‚Çä, 1)); @assert(size(X‚Çä) == size(ùõÉ))


    # pre-calculate
    
    inv_K_plus_Œõ‚Åª¬π = aux_invert_K_plus_Œõ‚Åª¬π(K=K, Œõroot=Œõroot)

    inv_K_mul_Œº·µÄ = (K\Œº')


    #--------------------------------------------------
    function unpack(p)
    #--------------------------------------------------

        local MARK = 0

        local Z‚Çä = reshape(p[MARK+1:MARK+Q*N‚Çä], Q, N‚Çä); MARK += Q*N‚Çä
        
        local Lroot = Diagonal(p[MARK+1:MARK+N‚Çä]); MARK += N‚Çä
        
        @assert(MARK == length(p)) # all parameters must be used up

        local ŒΩ  = net(w, Z‚Çä)

        return Z‚Çä, ŒΩ, Lroot

    end


    #--------------------------------------------------
    function objective(Z‚Çä, ŒΩ, Lroot)
    #--------------------------------------------------

        # Calculate cross-covariance matrix between test and training inputs
        local K‚Çä = covariance(pairwise(SqEuclidean(), Z‚Çä, Z), Œ∏); @assert(size(K‚Çä, 1) == N‚Çä)

        # Calculate "self"-covariance matrix between test inputs
        local K‚Çä‚Çä = Symmetric(covariance(pairwise(SqEuclidean(), Z‚Çä), Œ∏) + JITTER*I); @assert(size(K‚Çä‚Çä, 1) == N‚Çä)
        
        # calculate mean of "prior" of test latent function values
        local m = (K‚Çä*inv_K_mul_Œº·µÄ)'

        # calculate covariance of "prior" of test latent function values
        local C = K‚Çä‚Çä - K‚Çä*inv_K_plus_Œõ‚Åª¬π*K‚Çä'; @assert(size(C, 1) == N‚Çä); @assert(size(C, 2) == N‚Çä);

        local C·µ§ = cholesky(Symmetric(C)).L

        # calculate posterior covariance of test latent function values
        local A = aux_invert_K‚Åª¬π_plus_Œõ(K=Symmetric(C+JITTER*I) , Œõroot = Lroot)

        
        # log-prior contribution
        local ‚Ñì = -0.5*D*N‚Çä*log(2œÄ) - 0.5*sum(abs2.(C·µ§\(ŒΩ-m)')) - D*0.5*tr(C\A) - D*sum(log.(diag(C·µ§)))

        # code below implements line above - keep for numerical verification
        # let
        #     ‚Ñì1 = 0
        #     for d in 1:D
        #         ‚Ñì1 += logpdf(MvNormal(ŒΩ[d,:], Symmetric(C)), m[d,:]) - 0.5*tr(C\A)
        #     end
        # end

        # log-likelihood contribution
        
        local E, V = expectation_latent_function_values(;Œ± = Œ±, b = b, Œº = ŒΩ, Œ£ = A)

        ‚Ñì += -0.5*D*N‚Çä*log(2œÄ) + 0.5*sum(log.(ùõÉ))  -0.5*sum(ùõÉ .* abs2.(myskip.(X‚Çä .- E))) - 1/2 * sum(ùõÉ .* V)


        # entropy contribution with constants discarded
        ‚Ñì += 0.5*D*logabsdet(A)[1] 

        # penalty on latent - not in latex
        ‚Ñì += - 0.5*Œ∑*sum(abs2.(Z‚Çä))

    end



    # initialise parameters randomly
    @assert(N‚Çä == 1)

    function p0()

        local luckyindex = ceil(Int, rand(rg)*(size(Z,2))) 
        
        [Z[:,luckyindex]; randn(rg, N‚Çä)]

    end

    #-----------------------------------------------------------------
    # define options, loss and gradient to be passed to Optim.optimize
    #-----------------------------------------------------------------

    opt = Optim.Options(iterations = iterations, show_trace = false, show_every = 1)

    objective(p) = -objective(unpack(p)...)

    function fg!(F, G, x)
            
        value, ‚àáf = Zygote.withgradient(objective,x)

        isnothing(G) || copyto!(G, ‚àáf[1])

        isnothing(F) || return value

        nothing

    end


    #-----------------------------------------------------------------
    # Carry out actual optimisation and obtain optimised parameters
    #-----------------------------------------------------------------

    @printf("Optimising %d number of parameters\n",length(p0()))

    solutions = [optimize(Optim.only_fg!(fg!), p0(), ConjugateGradient(), opt) for _ in 1:repeats] # alphaguess = InitialQuadratic(Œ±0=1e-8)

    bestindex = argmin([s.minimizer for s in solutions])

    Zopt, ŒΩopt, Lroot = unpack(solutions[bestindex].minimizer)
   
    return Zopt

end