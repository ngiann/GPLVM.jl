function marginallikelihood_VERIFY(::Val{:gplvmvarnet_pos}, X, Z, Œ∏, ùõÉ, Œº, Œõroot, w, Œ±, b; JITTER = JITTER, Œ∑=Œ∑)

    local N = size(Œõroot, 1); @assert(size(Œº, 2) == size(Z, 2) == size(X, 2) == N)

    local D = size(X, 1); @assert(size(Œº, 1) == D)
    
    local D¬≤ = pairwise(SqEuclidean(), Z)

    local K  = Symmetric(covariance(D¬≤, Œ∏) + JITTER*I)

    local Œ£  = aux_invert_K‚Åª¬π_plus_Œõ(;K = K, Œõroot = Œõroot)+ JITTER*I

    local ‚Ñì = zero(eltype(Z))


    # log-prior contribution - implements equation eq:log_prior_gplvm_pos
    let 
        prior = MvNormal(zeros(N), K)
        for d in 1:D
            ‚Ñì += logpdf(prior, Œº[d,:]) - 0.5*tr(K\Œ£) 
        end
    end


    # log-likelihood contribution - implements eq:log_likel_gplvm_pos
    
    for n in 1:N, d in 1:D
        
        ‚Ñì += logpdf(Normal(E(a = Œ±, Œº = Œº[d,n], œÉ = sqrt(Œ£[n,n]), b = b), 1/sqrt(ùõÉ[d,n])), X[d,n]) - 
                0.5*ùõÉ[d,n]*V(a = Œ±, Œº = Œº[d,n], œÉ = sqrt(Œ£[n,n]), b = b)
    
    end

    # entropy contribution with constants discarded - implements eq:entropy_gplvm_pos

    ‚Ñì += D * Distributions.entropy(MvNormal(zeros(N), Œ£))

    # penalty on latent coordinates - not in latex

    ‚Ñì += - 0.5*Œ∑*sum(abs2.(Z))

    # penalty on neural network weights to smoothen optimisation - not in latex

    ‚Ñì += - 0.5*Œ∑*sum(abs2.(w))

    return ‚Ñì

end
