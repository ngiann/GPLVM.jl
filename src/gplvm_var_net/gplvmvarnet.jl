function gplvmvarnet(X, ğ›” = missing; iterations = 1, Î· = 1e-2, seed = 1, Q = 2, JITTER = 1e-6,  H1 = 10, H2 = H1, VERIFY = false)

    rg = MersenneTwister(seed)

    ğ›ƒ = inverterrors(ğ›”)

    D, N = size(X)
    
    net = ThreeLayerNetwork(in = Q, H1 = H1, H2 = H2, out=D)
    
    nwts = ForwardNeuralNetworks.numweights(net)
    
    @printf("Running gplvmvarnet.\n")
    @printf("There are %d number of data items\n", N)
    @printf("There are %d number of dimensions\n", D)
    @printf("Q=%d\n", Q)
    

    #-------------------------------------------
    function unpack(p, ::Missing)
    #-------------------------------------------

        @assert(length(p) == Q*N + 2 + 1 + nwts + N)

        local MARK = 0

        local Z = reshape(p[MARK+1:MARK+Q*N], Q, N); MARK += Q*N

        local Î¸ = exp.(p[MARK+1:MARK+2]); MARK += 2

        local Î² = exp(p[MARK+1]); MARK += 1

        local w = p[MARK+1:MARK+nwts]; MARK += nwts

        local Î›root = Diagonal(reshape(p[MARK+1:MARK+N], N)); MARK += N

        @assert(MARK == length(p))

        local Î¼ = net(w, Z)

        return Z, Î¸, Fill(Î², D, N), Î¼, Î›root

    end


    #-------------------------------------------
    function unpack(p, ğ›ƒ)
    #-------------------------------------------
    
        @assert(length(p) == Q*N + 2 + nwts + N)

        local MARK = 0

        local Z = reshape(p[MARK+1:MARK+Q*N], Q, N); MARK += Q*N

        local Î¸ = exp.(p[MARK+1:MARK+2]); MARK += 2

        local w = p[MARK+1:MARK+nwts]; MARK += nwts

        local Î›root = Diagonal(reshape(p[MARK+1:MARK+N], N)); MARK += N

        @assert(MARK == length(p))

        local Î¼ = net(w, Z)

        return Z, Î¸, ğ›ƒ, Î¼, Î›root

    end


    
    # Initialise parameters randomly

    p0 = let 
        
        ismissing(ğ›ƒ) ? [randn(rg, Q*N)*0.2; randn(rg,3)*1; 0.1*randn(rg, nwts); randn(rg, N)] :
        [randn(rg, Q*N)*0.2; randn(rg,2)*1; 0.1*randn(rg, nwts); randn(rg, N)]
        
    end
    
    @printf("Optimising %d number of parameters\n",length(p0))
    
    objective(p) = -marginallikelihood_gplvm_var_net(X, unpack(p, ğ›ƒ)...; JITTER = JITTER, Î· = Î·)

    # numerically verify before optimisation

    if VERIFY
        tmp1 = marginallikelihood_gplvm_var_net(X, unpack(p0, ğ›ƒ)...; JITTER = JITTER, Î· = Î·)
        tmp2 = marginallikelihood_gplvm_var_net_VERIFY(X, unpack(p0, ğ›ƒ)...; JITTER = JITTER, Î· = Î·)
        @printf("Verifiying calculations\n")
        @printf("First implementation delivers  %f\n", tmp1)
        @printf("Second implementation delivers %f\n", tmp2)
        @printf("difference is %f\n", tmp1-tmp2)
    end

    opt = Optim.Options(iterations = iterations, show_trace = true, show_every = 1)

    function fg!(F, G, x)
            
        value, âˆ‡f = Zygote.withgradient(objective, x)

        isnothing(G) || copyto!(G, âˆ‡f[1])

        isnothing(F) || return value

        nothing

    end

    results = optimize(Optim.only_fg!(fg!), p0, LBFGS(), opt)

    # numerically verify after optimisation

    if VERIFY
        tmp1 =        marginallikelihood_gplvm_var_net(X, unpack(p0, ğ›ƒ)...; JITTER = JITTER, Î· = Î·)
        tmp2 = marginallikelihood_gplvm_var_net_VERIFY(X, unpack(p0, ğ›ƒ)...; JITTER = JITTER, Î· = Î·)
        @printf("Verifiying calculations\n")
        @printf("First implementation delivers  %f\n", tmp1)
        @printf("Second implementation delivers %f\n", tmp2)
        @printf("difference is %f\n", tmp1-tmp2)
    end

    Zopt, Î¸opt, ğ›ƒopt, Î¼opt, Î›rootopt = unpack(results.minimizer, ğ›ƒ)
 
    Kopt = let

        local DÂ² = pairwise(SqEuclidean(), Zopt)

        Symmetric(covariance(DÂ², Î¸opt) + JITTER*I)

    end

    return (Z = Zopt, Î¸ = Î¸opt, ğ›ƒ = ğ›ƒopt, Î¼ = Î¼opt, Î›root = Î›rootopt, K = Kopt, JITTER = JITTER)

end